GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 9, MEMBER: 10/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 8, MEMBER: 9/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 12, MEMBER: 13/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 13, MEMBER: 14/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 14, MEMBER: 15/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 15, MEMBER: 16/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 21, MEMBER: 22/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 17, MEMBER: 18/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 10, MEMBER: 11/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 16, MEMBER: 17/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 20, MEMBER: 21/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 11, MEMBER: 12/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 25, MEMBER: 26/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 24, MEMBER: 25/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 23, MEMBER: 24/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 22, MEMBER: 23/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 18, MEMBER: 19/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 19, MEMBER: 20/26
----------------------------------------------------------------------------------------------------
distributed_backend=ddp
All DDP processes registered. Starting ddp with 26 processes
----------------------------------------------------------------------------------------------------
/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.

  | Name     | Type     | Params
--------------------------------------
0 | cnntornn | CNNtoRNN | 24 M  
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..


Profiler Report

Action              	|  Mean duration (s)	|  Total time (s) 
-----------------------------------------------------------------
on_train_start      	|  0.007111       	|  0.007111       
on_epoch_start      	|  0.0003255      	|  0.09765        
on_train_epoch_start	|  1.5738e-05     	|  0.0047213      
get_train_batch     	|  0.16025        	|  1009.6         
on_batch_start      	|  2.344e-05      	|  0.14064        
on_train_batch_start	|  1.2594e-05     	|  0.075565       
model_forward       	|  0.030332       	|  181.99         
model_backward      	|  0.11873        	|  712.37         
on_after_backward   	|  2.7176e-06     	|  0.016306       
optimizer_step      	|  0.0022285      	|  13.371         
on_batch_end        	|  2.2518e-05     	|  0.13511        
on_train_batch_end  	|  0.00053151     	|  3.189          
on_epoch_end        	|  2.7777e-05     	|  0.0083332      
on_train_epoch_end  	|  1.2166e-05     	|  0.0036499      
on_train_end        	|  0.00034692     	|  0.00034692     

