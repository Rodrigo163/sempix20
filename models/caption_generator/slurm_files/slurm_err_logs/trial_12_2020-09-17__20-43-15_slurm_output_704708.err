GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 9, MEMBER: 10/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 8, MEMBER: 9/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 13, MEMBER: 14/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 12, MEMBER: 13/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 17, MEMBER: 18/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 16, MEMBER: 17/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 21, MEMBER: 22/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 22, MEMBER: 23/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 23, MEMBER: 24/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 20, MEMBER: 21/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 24, MEMBER: 25/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 25, MEMBER: 26/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 19, MEMBER: 20/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 18, MEMBER: 19/26
GPU available: True, used: True
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/26
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 14, MEMBER: 15/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 15, MEMBER: 16/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 10, MEMBER: 11/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 11, MEMBER: 12/26
GPU available: True, used: True
----------------------------------------------------------------------------------------------------
distributed_backend=ddp
All DDP processes registered. Starting ddp with 26 processes
----------------------------------------------------------------------------------------------------
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/26
/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.

  | Name     | Type     | Params
--------------------------------------
0 | cnntornn | CNNtoRNN | 24 M  
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..


Profiler Report

Action              	|  Mean duration (s)	|  Total time (s) 
-----------------------------------------------------------------
on_train_start      	|  0.0031543      	|  0.0031543      
on_epoch_start      	|  0.0002866      	|  0.085981       
on_train_epoch_start	|  1.2051e-05     	|  0.0036152      
get_train_batch     	|  0.045192       	|  542.3          
on_batch_start      	|  1.7271e-05     	|  0.20208        
on_train_batch_start	|  1.1375e-05     	|  0.13309        
model_forward       	|  0.03065        	|  358.6          
model_backward      	|  0.015029       	|  175.84         
on_after_backward   	|  2.2247e-06     	|  0.026029       
optimizer_step      	|  0.0025649      	|  30.01          
on_batch_end        	|  2.303e-05      	|  0.26945        
on_train_batch_end  	|  0.00042787     	|  5.006          
on_epoch_end        	|  2.2108e-05     	|  0.0066324      
on_train_epoch_end  	|  1.0204e-05     	|  0.0030613      
on_train_end        	|  0.0011632      	|  0.0011632      

