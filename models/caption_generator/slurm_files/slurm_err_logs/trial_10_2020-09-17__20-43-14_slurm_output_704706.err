GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 24, MEMBER: 25/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 21, MEMBER: 22/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 20, MEMBER: 21/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 9, MEMBER: 10/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 25, MEMBER: 26/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 13, MEMBER: 14/26
GPU available: True, used: True
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 8, MEMBER: 9/26
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/26
GPU available: True, used: True
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 12, MEMBER: 13/26
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 16, MEMBER: 17/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 17, MEMBER: 18/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/26
GPU available: True, used: True
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 18, MEMBER: 19/26
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 15, MEMBER: 16/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 19, MEMBER: 20/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 23, MEMBER: 24/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 22, MEMBER: 23/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 14, MEMBER: 15/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 10, MEMBER: 11/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 11, MEMBER: 12/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/26
----------------------------------------------------------------------------------------------------
distributed_backend=ddp
All DDP processes registered. Starting ddp with 26 processes
----------------------------------------------------------------------------------------------------
/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.

  | Name     | Type     | Params
--------------------------------------
0 | cnntornn | CNNtoRNN | 24 M  
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..
Saving latest checkpoint..


Profiler Report

Action              	|  Mean duration (s)	|  Total time (s) 
-----------------------------------------------------------------
on_train_start      	|  0.0050035      	|  0.0050035      
on_epoch_start      	|  0.00031464     	|  0.094393       
on_train_epoch_start	|  1.411e-05      	|  0.0042329      
get_train_batch     	|  0.16218        	|  1021.7         
on_batch_start      	|  2.4553e-05     	|  0.14732        
on_train_batch_start	|  1.1574e-05     	|  0.069444       
model_forward       	|  0.031882       	|  191.29         
model_backward      	|  0.11817        	|  709.01         
on_after_backward   	|  2.7381e-06     	|  0.016429       
optimizer_step      	|  0.0031407      	|  18.844         
on_batch_end        	|  2.1706e-05     	|  0.13024        
on_train_batch_end  	|  0.00053849     	|  3.2309         
on_epoch_end        	|  2.7946e-05     	|  0.0083837      
on_train_epoch_end  	|  9.9393e-06     	|  0.0029818      
on_train_end        	|  0.00033784     	|  0.00033784     

