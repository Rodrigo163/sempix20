Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 42, in main
    trainer = Trainer(logger = logger,
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 507, in __init__
    self.gpus = pick_multiple_gpus(gpus)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 474, in pick_multiple_gpus
    picked.append(pick_single_gpu(exclude_gpus=picked))
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 468, in pick_single_gpu
    raise RuntimeError("No GPUs available.")
RuntimeError: No GPUs available.
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 42, in main
    trainer = Trainer(logger = logger,
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 507, in __init__
    self.gpus = pick_multiple_gpus(gpus)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 474, in pick_multiple_gpus
    picked.append(pick_single_gpu(exclude_gpus=picked))
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 468, in pick_single_gpu
    raise RuntimeError("No GPUs available.")
RuntimeError: No GPUs available.
GPU available: True, used: True
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 13, MEMBER: 14/26
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 25, MEMBER: 26/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 24, MEMBER: 25/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 12, MEMBER: 13/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 10, MEMBER: 11/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 21, MEMBER: 22/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 20, MEMBER: 21/26
GPU available: True, used: True
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 17, MEMBER: 18/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 16, MEMBER: 17/26
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 23, MEMBER: 24/26
GPU available: True, used: True
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 11, MEMBER: 12/26
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 22, MEMBER: 23/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 18, MEMBER: 19/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 19, MEMBER: 20/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 15, MEMBER: 16/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 14, MEMBER: 15/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/26
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Multi-processing is handled by Slurm.
CUDA_VISIBLE_DEVICES: [0,1]
initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/26
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
----------------------------------------------------------------------------------------------------
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
distributed_backend=ddp
All DDP processes registered. Starting ddp with 26 processes
----------------------------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
Traceback (most recent call last):
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/test_tube/hpc.py", line 269, in __run_experiment
    train_function(self.hyperparam_optimizer, self)
  File "/usr/data/bgfs1/rlopez/ic_sempix/grid_search/pl_grid.py", line 51, in main
    trainer.fit(model, dm)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1041, in fit
    self.accelerator_backend.train(model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 57, in train
    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py", line 221, in ddp_train
    model = model.configure_ddp(model, device_ids)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 837, in configure_ddp
    model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 331, in __init__
    self._distributed_broadcast_coalesced(
  File "/usr/home/rlopez/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 549, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
RuntimeError: Socket Timeout
srun: error: node101: task 0: User defined signal 1
srun: error: node116: task 5: User defined signal 1
srun: error: node134: task 6: User defined signal 1
srun: error: node116: task 4: User defined signal 1
srun: error: node134: task 7: User defined signal 1
